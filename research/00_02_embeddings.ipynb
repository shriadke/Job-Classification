{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings research\n",
    "\n",
    "### Purpose\n",
    "\n",
    "1. To load the given Sentence embedding models and test on given data,\n",
    "2. Identify suitability of different SBERT models for given task and select the relevant,\n",
    "3. Generate training, validation and testing datasets for finetuning a custom SBERT model,\n",
    "4. Define and test the custom/reusable data embedding methods,\n",
    "5. Train and store custom SBERT model. Prepare training pipeline for the future,\n",
    "6. Test the custom SBERT-based embedding with semantic search as a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shrin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train and test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../artifacts/data_ingestion/raw_train/train_data.csv\")\n",
    "train_df = pd.read_csv(\"../artifacts/data_ingestion/raw_train/train_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Common Functions for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_special_tokens(path=\"../data/raw/special_words.txt\"):\n",
    "    \"\"\" \n",
    "    This method returns list of special words to be removed from the text.\n",
    "    \n",
    "    Input: path: str --> Path to the word file\n",
    "\n",
    "    Output: : special_tokens: list[str] --> all words/special tokens to be removed\n",
    "    \"\"\"\n",
    "    special_tokens=[]\n",
    "    if not os.path.exists(path):\n",
    "        return []\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        special_tokens = f.readlines()\n",
    "    special_tokens = [line.rstrip('\\n') for line in special_tokens]\n",
    "    \n",
    "    return special_tokens\n",
    "\n",
    "def clean_text(text, stop_words=stopwords.words('english'), punct=string.punctuation, special_tokens=[]):\n",
    "        \"\"\" \n",
    "        This method returns cleaned String from an input string. \n",
    "        Removes stop words, punctuations, numbers and any special tokens given.\n",
    "        \n",
    "        Input:  text : str                  --> input string to be cleaned\n",
    "                stop_words : list[str]      --> (Optional) list of stop words to be removed from the text\n",
    "                punct : list[str]           --> (Optional) list of punctuations to be removed from the text\n",
    "                special_tokens : list[str]  --> (Optional) list of special words to be removed from the text\n",
    "\n",
    "        Output: text: string:  cleaned text\n",
    "        \"\"\"\n",
    "\n",
    "        text= text.lower()\n",
    "        \n",
    "        text = text.replace(\"\\n\",\" \")\n",
    "        \n",
    "        text = text.replace(r'[0-9]+', ' ')\n",
    "        text = text.replace(r'[^\\w\\s]', ' ')\n",
    "        text = text.replace(r'[^a-zA-Z]', ' ')\n",
    "        for p in punct:\n",
    "            text = text.replace(p,\" \") \n",
    "            \n",
    "        text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "        text = ' '.join([word for word in text.split() if word not in special_tokens])\n",
    "        text = ''.join([i for i in text if not i.isdigit()])\n",
    "        text = text.replace(r'\\s+', ' ')\n",
    "        text = ' '.join([i for i in text.split() if len(i)>1])\n",
    "        \n",
    "        text = text.replace(r'\\s+', ' ')\n",
    "        return text\n",
    "\n",
    "def get_clean_job_str(job_title, job_post):\n",
    "    \"\"\" \n",
    "    This method returns cleaned Job Posting from Job Title and Job String. \n",
    "    Appends title and body tokens and concatenates the two.\n",
    "    \n",
    "    Input: Job Title Raw : string\n",
    "            Job Body Raw : string\n",
    "    Output: job_str: string:  cleaned and concatenated job details\n",
    "    \"\"\"\n",
    "    title_token = \"[TTL] \"\n",
    "    body_token = \" [DESC] \"\n",
    "\n",
    "    job_title = clean_text(job_title, special_tokens=get_special_tokens())\n",
    "    job_post = clean_text(job_post, special_tokens=get_special_tokens())\n",
    "\n",
    "    job_str = title_token + job_title + body_token + job_post\n",
    "\n",
    "    return job_str\n",
    "def get_all_onets(onet_data_path=\"../data/raw/All_Occupations.csv\"):\n",
    "    \"\"\" \n",
    "    This method returns list of all ONETs available on the official site\n",
    "    \n",
    "    Input: path: str -->  (Optional) Path to the onet csv file\n",
    "\n",
    "    Output: : all_onets_original: list[str] --> all ONETs available\n",
    "    \"\"\"\n",
    "    all_occupations_df = pd.read_csv(onet_data_path)\n",
    "    all_onets_original = all_occupations_df.Occupation.to_list()\n",
    "    return all_onets_original\n",
    "\n",
    "def get_onet_dicts(all_onets_original=get_all_onets()):\n",
    "    \"\"\" \n",
    "    This method returns 2 dictionaries used to map standard ONET Names to string IDs. \n",
    "    \n",
    "    Input: all_onets_original: list[str] --> (Optional) list of all ONETs\n",
    "\n",
    "    Output: : id_to_onet_dict: dict[str, str] --> standard mapping of string id to ONETs --> \"id\" : \"ONET_NAME\"\n",
    "              onet_to_id_dict: dict[str, str] --> standard mapping of ONETs to string id --> \"ONET_NAME\" : \"id\"\n",
    "    \"\"\"\n",
    "    id_to_onet_dict = {str(id):onet for id, onet in enumerate(all_onets_original)}\n",
    "    onet_to_id_dict = {onet:id for id,onet in id_to_onet_dict.items()}\n",
    "    return id_to_onet_dict, onet_to_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Sentence embeddings using SBERT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed, AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset, load_from_disk, load_metric\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, models, InputExample, losses, evaluation, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embd(input_docs, model=None, model_ckpt=None, save_embd=False, save_path=None):\n",
    "    \"\"\" \n",
    "    This method computes embedding of the given string or list of strings using the given SBERT model. \n",
    "    \n",
    "    Input: input_docs: str or list[str] --> list of input string. Can be a single string which will be converted to a list.\n",
    "           model: SentenceTransformers() --> (Optional) pre-trained SBERT model, if None, checks for path \n",
    "           model_ckpt: str --> (Optional) pre-trained SBERT model checkpoint path, if None, loads basic SBERT from HF library \n",
    "           save_embd: Boolean --> (Optional) Flag to save computed embeddings.\n",
    "           save_path: str --> (Optional) Path to save computed embeddings. If empty, new path will be created\n",
    "\n",
    "    Output: simple_embd: numpy.ndarray (len(input), embd_size) --> array of Computed sentence embeddings. \n",
    "    \"\"\"\n",
    "\n",
    "    if not input_docs:\n",
    "        return None\n",
    "    if not isinstance(input_docs, list):\n",
    "        print(\"convert string to list\")\n",
    "        input_docs = [input_docs]       \n",
    "\n",
    "    if model:\n",
    "        print(\"loading model as it is\")\n",
    "        sbert_model = model\n",
    "    elif model_ckpt:\n",
    "        if not os.path.exists(model_ckpt):\n",
    "            model_ckpt = \"shriadke/adept-job-msmarco-distilbert-base-v4\"\n",
    "        print(\"loading model from ckpt: \", model_ckpt)\n",
    "        sbert_model = SentenceTransformer(model_ckpt)\n",
    "    else:\n",
    "        print(\"loading HF base model\")\n",
    "        sbert_model = SentenceTransformer(\"msmarco-distilbert-base-v4\")\n",
    "    \n",
    "    simple_embd = sbert_model.encode(input_docs, show_progress_bar=True)\n",
    "    \n",
    "    if save_embd and save_path:\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            #os.chmod(save_path+\"embd.pkl\", 0o777)\n",
    "        with open(save_path+\"embd.pkl\", \"wb\") as fOut:\n",
    "            print(\"lenght of docs: \", len(input_docs))\n",
    "            print(\"lenght of embd: \", len(simple_embd))\n",
    "            pickle.dump({'input': input_docs, 'embeddings': simple_embd}, fOut, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return simple_embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert string to list\n",
      "loading HF base model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a838e0466e4a2d97401ceae96aade4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O*NET:  Accountants and Auditors  is converted to a vector of shape:  (1, 768)\n"
     ]
    }
   ],
   "source": [
    "# Testing get embeddings\n",
    "id_to_onet_dict, onet_to_id_dict = get_onet_dicts()\n",
    "all_onets_original = get_all_onets()\n",
    "print(\"O*NET: \", all_onets_original[0], \" is converted to a vector of shape: \", get_embd(all_onets_original[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onet_embeddings(model=None, model_ckpt=None, onet_embd_path=None, save_embd=False, save_path=\"../data/processed/embeddings/onet/\"):\n",
    "    \"\"\" \n",
    "    This method computes embedding for all ONETs available. \n",
    "    \n",
    "    Input: model: SentenceTransformers() --> (Optional) pre-trained SBERT model, if None, checks for path \n",
    "           model_ckpt: str --> (Optional) pre-trained SBERT model checkpoint path, if None, loads basic SBERT from HF library \n",
    "           onet_embd_path: str --> (Optional) Path to load computed embeddings from pickle. If empty, Embeddings will be computed from scratch. \n",
    "           save_embd: Boolean --> (Optional) Flag to save computed embeddings.\n",
    "           save_path: str --> (Optional) Path to save computed embeddings. If empty, new path will be created\n",
    "\n",
    "    Output: onet_embd_df: pandas.DataFrame --> Dataframe with 2 columns:[\"ONET_NAME\", \"ONET_EMBD\"] \n",
    "                                               Computed sentence embeddings can be stored as key, val pair. \n",
    "    \"\"\"\n",
    "    onet_embd_df = pd.DataFrame(columns=[\"ONET_NAME\", \"ONET_EMBD\"])\n",
    "    \n",
    "    if not onet_embd_path:\n",
    "        # Create onet embeddings from all onet data\n",
    "        # Get list of all ONETs\n",
    "        all_onets_original = get_all_onets()\n",
    "\n",
    "        # Compute Embeddings of the entire list \n",
    "        simple_embd = get_embd(all_onets_original, model=model, model_ckpt=model_ckpt, save_embd=save_embd, save_path=save_path)\n",
    "\n",
    "        # Save Embds as dataframe\n",
    "        onet_embd_df[\"ONET_NAME\"] = pd.Series(all_onets_original)\n",
    "        onet_embd_df[\"ONET_EMBD\"] = pd.Series([arr for arr in simple_embd])\n",
    "        \n",
    "    elif os.path.exists(onet_embd_path):\n",
    "        #Load sentences & embeddings from disc\n",
    "        with open(onet_embd_path, \"rb\") as fIn:\n",
    "            stored_data = pickle.load(fIn)\n",
    "            onet_embd_df[\"ONET_NAME\"] = stored_data['input']\n",
    "            onet_embd_df[\"ONET_EMBD\"] = pd.Series([arr for arr in stored_data['embeddings']])\n",
    "    \n",
    "    print(\"Total ONETs available: \",len(onet_embd_df))\n",
    "    return onet_embd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_embed_df_from_df(job_df=None,model=None, model_ckpt=None, job_embd_path=None, save_embd=False, save_path=\"../data/processed/embeddings/job/\"):\n",
    "    \"\"\" \n",
    "    This method computes embedding for all ONETs available. \n",
    "    \n",
    "    Input: job_df: pandas.DataFrame --> (Optional) Raw job data Dataframe with at least 2 columns:[\"TITLE_RAW\", \"BODY\"], if empty, loads precomputed.\n",
    "           model: SentenceTransformers() --> (Optional) pre-trained SBERT model, if None, checks for path \n",
    "           model_ckpt: str --> (Optional) pre-trained SBERT model checkpoint path, if None, loads basic SBERT from HF library \n",
    "           job_embd_path: str --> (Optional) Path to load computed embeddings from pickle. If empty, Embeddings will be computed from scratch. \n",
    "           save_embd: Boolean --> (Optional) Flag to save computed embeddings.\n",
    "           save_path: str --> (Optional) Path to save computed embeddings. If empty, new path will be created\n",
    "\n",
    "    Output: onet_embd_df: pandas.DataFrame --> Dataframe with 2 columns:[\"ONET_NAME\", \"ONET_EMBD\"] \n",
    "                                               Computed sentence embeddings can be stored as key, val pair. \n",
    "    \"\"\"\n",
    "    if job_df is None:\n",
    "        print(\"Path to Job DF Given\")\n",
    "        job_df = pd.DataFrame(columns=[\"TITLE_RAW\",\"BODY\", \"CLEANED_JOB\", \"JOB_EMBD\"])\n",
    "        # load embeddings from stored DF embeddings\n",
    "        if os.path.exists(job_embd_path):\n",
    "            #Load sentences & embeddings from disc\n",
    "            with open(job_embd_path, \"rb\") as fIn:\n",
    "                stored_data = pickle.load(fIn)\n",
    "                # Loads cleaned job str and its embeddings\n",
    "                job_df[\"CLEANED_JOB\"] = stored_data['input']\n",
    "                job_df[\"JOB_EMBD\"] = pd.Series([arr for arr in stored_data['embeddings']])\n",
    "                job_df[\"TITLE_RAW\"] = job_df[\"CLEANED_JOB\"].apply(lambda x:x[6:x.find(\" [DESC] \")])\n",
    "                job_df[\"BODY\"] = job_df[\"CLEANED_JOB\"].apply(lambda x:x[x.find(\" [DESC] \")+1:])\n",
    "        else:\n",
    "            print(\"Path to Job DF does not exists\")\n",
    "            return job_df\n",
    "    elif len(job_df) > 0:\n",
    "        # DF present, compute from Raw DF\n",
    "        if not (\"TITLE_RAW\" in job_df.columns and \"BODY\" in job_df.columns):\n",
    "            print(\"Incomplete DataFrame, please try again\")\n",
    "            return None\n",
    "        if not \"CLEANED_JOB\" in job_df.columns:\n",
    "            job_df[\"CLEANED_JOB\"] = job_df.apply(lambda x:get_clean_job_str(x[\"TITLE_RAW\"], x[\"BODY\"]), axis=1)\n",
    "        \n",
    "        simple_embd = get_embd(job_df[\"CLEANED_JOB\"].to_list(), model=model, model_ckpt=model_ckpt, save_embd=save_embd, save_path=str(save_path)+str(len(job_df))+\"/\")\n",
    "\n",
    "        job_df[\"JOB_EMBD\"] = pd.Series([arr for arr in simple_embd])\n",
    "    else:\n",
    "        print(\"Unexpected Input Job DF, please try again\")\n",
    "    \n",
    "    print(\"Total Jobs available: \",len(job_df)) \n",
    "    return job_df\n",
    "\n",
    "def get_job_embd_df_frm_title_body(job_title, job_body, model=None, model_ckpt=None):\n",
    "    job_df = pd.DataFrame({ \"TITLE_RAW\" : [job_title],\n",
    "                            \"BODY\"      : [job_body], })\n",
    "                            #\"CLEANED_JOB\": get_clean_job_str(job_title, job_body)\n",
    "    job_df = get_job_embed_df_from_df(job_df=job_df, model=model, model_ckpt=model_ckpt)\n",
    "\n",
    "    return job_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading HF base model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc438d3a2e82495da9a4f16e868f035b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Jobs available:  5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>POSTED</th>\n",
       "      <th>TITLE_RAW</th>\n",
       "      <th>BODY</th>\n",
       "      <th>ONET_NAME</th>\n",
       "      <th>ONET</th>\n",
       "      <th>CLEANED_JOB</th>\n",
       "      <th>JOB_EMBD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3a9bc988d77e46507f6753429dd848a816d0b9b9</td>\n",
       "      <td>2023-05-03</td>\n",
       "      <td>Executive Meeting Manager</td>\n",
       "      <td>Executive Meeting Manager Marriott La Jolla - ...</td>\n",
       "      <td>Meeting, Convention, and Event Planners</td>\n",
       "      <td>13-1121.00</td>\n",
       "      <td>[TTL] executive meeting manager [DESC] executi...</td>\n",
       "      <td>[-0.08524054, 0.20759732, 0.20139317, -0.12825...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eb3a017370d55577e892ff8207a640b7d7136f31</td>\n",
       "      <td>2023-05-03</td>\n",
       "      <td>Rehabilitation Technician-Outpatient Rehab-Fle...</td>\n",
       "      <td>Rehabilitation Technician-Outpatient Rehab-Fle...</td>\n",
       "      <td>Occupational Therapy Aides</td>\n",
       "      <td>31-2012.00</td>\n",
       "      <td>[TTL] rehabilitation technician outpatient reh...</td>\n",
       "      <td>[0.13869289, -0.4502431, 0.76077837, -0.099650...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8717d2213055d39271bd12490263a7fbe603aedb</td>\n",
       "      <td>2023-05-03</td>\n",
       "      <td>Office/Bookkeeping Assistant</td>\n",
       "      <td>Office/Bookkeeping Assistant\\nSanta Barbara, C...</td>\n",
       "      <td>Office Clerks, General</td>\n",
       "      <td>43-9061.00</td>\n",
       "      <td>[TTL] office bookkeeping assistant [DESC] offi...</td>\n",
       "      <td>[0.51665556, 0.4805767, 0.16331044, -0.1097672...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43b55e4334835e20e1c64d9ac7bb0a0267369b9e</td>\n",
       "      <td>2023-05-03</td>\n",
       "      <td>Administrative Support Coordinator - VA - (REM...</td>\n",
       "      <td>Find Jobs Administrative Support Coordinator -...</td>\n",
       "      <td>Secretaries and Administrative Assistants, Exc...</td>\n",
       "      <td>43-6014.00</td>\n",
       "      <td>[TTL] administrative support coordinator va re...</td>\n",
       "      <td>[-0.22788213, -0.17873518, 0.07261607, -0.1571...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afa355a328687ddb88d6265a237d0375bb36eae7</td>\n",
       "      <td>2023-05-03</td>\n",
       "      <td>Receptionist/Administrative Assistant</td>\n",
       "      <td>Receptionist/Administrative Assistant Burgess ...</td>\n",
       "      <td>Secretaries and Administrative Assistants, Exc...</td>\n",
       "      <td>43-6014.00</td>\n",
       "      <td>[TTL] receptionist administrative assistant [D...</td>\n",
       "      <td>[0.33224586, 0.061779127, 0.43059358, -0.18563...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         ID      POSTED  \\\n",
       "0  3a9bc988d77e46507f6753429dd848a816d0b9b9  2023-05-03   \n",
       "1  eb3a017370d55577e892ff8207a640b7d7136f31  2023-05-03   \n",
       "2  8717d2213055d39271bd12490263a7fbe603aedb  2023-05-03   \n",
       "3  43b55e4334835e20e1c64d9ac7bb0a0267369b9e  2023-05-03   \n",
       "4  afa355a328687ddb88d6265a237d0375bb36eae7  2023-05-03   \n",
       "\n",
       "                                           TITLE_RAW  \\\n",
       "0                          Executive Meeting Manager   \n",
       "1  Rehabilitation Technician-Outpatient Rehab-Fle...   \n",
       "2                       Office/Bookkeeping Assistant   \n",
       "3  Administrative Support Coordinator - VA - (REM...   \n",
       "4              Receptionist/Administrative Assistant   \n",
       "\n",
       "                                                BODY  \\\n",
       "0  Executive Meeting Manager Marriott La Jolla - ...   \n",
       "1  Rehabilitation Technician-Outpatient Rehab-Fle...   \n",
       "2  Office/Bookkeeping Assistant\\nSanta Barbara, C...   \n",
       "3  Find Jobs Administrative Support Coordinator -...   \n",
       "4  Receptionist/Administrative Assistant Burgess ...   \n",
       "\n",
       "                                           ONET_NAME        ONET  \\\n",
       "0            Meeting, Convention, and Event Planners  13-1121.00   \n",
       "1                         Occupational Therapy Aides  31-2012.00   \n",
       "2                             Office Clerks, General  43-9061.00   \n",
       "3  Secretaries and Administrative Assistants, Exc...  43-6014.00   \n",
       "4  Secretaries and Administrative Assistants, Exc...  43-6014.00   \n",
       "\n",
       "                                         CLEANED_JOB  \\\n",
       "0  [TTL] executive meeting manager [DESC] executi...   \n",
       "1  [TTL] rehabilitation technician outpatient reh...   \n",
       "2  [TTL] office bookkeeping assistant [DESC] offi...   \n",
       "3  [TTL] administrative support coordinator va re...   \n",
       "4  [TTL] receptionist administrative assistant [D...   \n",
       "\n",
       "                                            JOB_EMBD  \n",
       "0  [-0.08524054, 0.20759732, 0.20139317, -0.12825...  \n",
       "1  [0.13869289, -0.4502431, 0.76077837, -0.099650...  \n",
       "2  [0.51665556, 0.4805767, 0.16331044, -0.1097672...  \n",
       "3  [-0.22788213, -0.17873518, 0.07261607, -0.1571...  \n",
       "4  [0.33224586, 0.061779127, 0.43059358, -0.18563...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating embeddings from dummy dataframe of 5 examples\n",
    "temp_train_df = get_job_embed_df_from_df(job_df=train_df.copy().head())\n",
    "temp_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading HF base model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d104399a10ed4990ad54425ec4ffdd3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Jobs available:  1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE_RAW</th>\n",
       "      <th>BODY</th>\n",
       "      <th>CLEANED_JOB</th>\n",
       "      <th>JOB_EMBD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Executive Meeting Manager</td>\n",
       "      <td>Executive Meeting Manager Marriott La Jolla - ...</td>\n",
       "      <td>[TTL] executive meeting manager [DESC] executi...</td>\n",
       "      <td>[-0.08524054, 0.20759732, 0.20139317, -0.12825...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   TITLE_RAW  \\\n",
       "0  Executive Meeting Manager   \n",
       "\n",
       "                                                BODY  \\\n",
       "0  Executive Meeting Manager Marriott La Jolla - ...   \n",
       "\n",
       "                                         CLEANED_JOB  \\\n",
       "0  [TTL] executive meeting manager [DESC] executi...   \n",
       "\n",
       "                                            JOB_EMBD  \n",
       "0  [-0.08524054, 0.20759732, 0.20139317, -0.12825...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating embeddings from Single record\n",
    "temp_train_df = get_job_embd_df_frm_title_body(train_df[\"TITLE_RAW\"][0],train_df[\"BODY\"][0])\n",
    "temp_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning SBERT for custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30524, 768)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained model \n",
    "model_ckpt = \"msmarco-distilbert-base-v4\"\n",
    "\n",
    "sbert_model = SentenceTransformer(model_ckpt)\n",
    "\n",
    "word_embedding_model = sbert_model._first_module()\n",
    "\n",
    "tokens = [\"[TTL] \", \" [DESC] \"]\n",
    "word_embedding_model.tokenizer.add_tokens(tokens, special_tokens=True)\n",
    "word_embedding_model.auto_model.resize_token_embeddings(len(word_embedding_model.tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation into train, val, test splits\n",
    "\n",
    "def get_dataloader(data, split):\n",
    "        examples = []\n",
    "        data = data[split]\n",
    "        n_examples = data.num_rows\n",
    "\n",
    "        for i in range(n_examples):\n",
    "            example = data[i]\n",
    "            examples.append(InputExample(texts=[example['CLEAN_JOB'], example['ONET_NAME']], label=float(1)))\n",
    "        logger.info(f\"in {split}, We have a {type(examples)} of length {len(examples)} containing {type(examples[0])}'s.\")\n",
    "        dataloader = DataLoader(examples, shuffle=True, batch_size=16)\n",
    "        return examples, dataloader\n",
    "\n",
    "def convert(self):\n",
    "\n",
    "    # get csv data to df\n",
    "    train_df = pd.read_csv(self.config.data_path+\"raw_train/train_data.csv\").head()\n",
    "    print(\"TITLE_RAW\"  in train_df.columns)\n",
    "    test_df = pd.read_csv(self.config.data_path+\"raw_test/test_data.csv\").head()\n",
    "\n",
    "    # add clean col to df\n",
    "    train_df[\"CLEAN_JOB\"] = train_df.apply(lambda x:get_clean_job_str(x[\"TITLE_RAW\"], x[\"BODY\"]), axis=1)\n",
    "    test_df[\"CLEAN_JOB\"] = test_df.apply(lambda x:get_clean_job_str(x[\"TITLE_RAW\"], x[\"BODY\"]), axis=1)\n",
    "\n",
    "    # split train/val/test data\n",
    "    train_ratio = 0.85\n",
    "    val_ratio = 0.15\n",
    "    train_df, val_df = train_test_split(train_df, test_size=1 - train_ratio, random_state=42, shuffle=True)\n",
    "\n",
    "    final_data = DatasetDict({\n",
    "        \"train\" : Dataset.from_pandas(train_df).remove_columns([\"__index_level_0__\"]),\n",
    "        \"val\" : Dataset.from_pandas(val_df).remove_columns([\"__index_level_0__\"]),\n",
    "        \"test\" : Dataset.from_pandas(test_df)\n",
    "    })\n",
    "    #final_data.save_to_disk( os.path.join(self.config.data_path,\"final_data/\"))\n",
    "\n",
    "    dataset = final_data#load_from_disk( os.path.join(self.config.data_path,\"final_data/\"))\n",
    "\n",
    "    \n",
    "    train_examples, train_dataloader = self.get_dataloader(dataset, \"train\")\n",
    "    torch.save(train_dataloader, os.path.join(self.config.root_dir,\"train.pth\"))\n",
    "\n",
    "    \n",
    "    val_examples, val_dataloader = self.get_dataloader(dataset, \"val\")\n",
    "    val_evaluator = evaluation.EmbeddingSimilarityEvaluator([],[],[]).from_input_examples(examples=val_examples)\n",
    "    torch.save(val_dataloader, os.path.join(self.config.root_dir,\"val.pth\"))\n",
    "    torch.save(val_evaluator, os.path.join(self.config.root_dir,\"val_eval.pth\"))\n",
    "\n",
    "    test_examples, test_dataloader = self.get_dataloader(dataset, \"test\")\n",
    "    test_evaluator = evaluation.EmbeddingSimilarityEvaluator([],[],[]).from_input_examples(examples=test_examples)\n",
    "    torch.save(test_dataloader, os.path.join(self.config.root_dir,\"test.pth\"))\n",
    "    torch.save(test_evaluator, os.path.join(self.config.root_dir,\"test_eval.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Transformation Moved to [`research/03_model_transformation.ipynb`](https://github.com/shriadke/JobClassification/blob/master/research/03_model_transformation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "val_dataloader = DataLoader(val_examples, shuffle=True, batch_size=16)\n",
    "test_dataloader = DataLoader(test_examples, shuffle=True, batch_size=16)\n",
    "\n",
    "# training Loss used when all examples are positive pairs\n",
    "loss = losses.MultipleNegativesRankingLoss(model=sbert_model)\n",
    "\n",
    "# TRAINING ARGS\n",
    "num_epochs = 2\n",
    "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1) #10% of train data\n",
    "weight_decay = 0.01\n",
    "output_path = \"./models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63919a9edb3a4879af1897377b35cb53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed42e6ee2464c23baeb5d417964f249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/953 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc28f1e6c5384deb9119e923390978aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/953 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# change test_dataloader while training\n",
    "sbert_model.fit(train_objectives=[(train_dataloader, loss)], epochs = num_epochs, warmup_steps= warmup_steps, weight_decay=weight_decay, output_path= output_path)\n",
    "\n",
    "# Here the model is trained on 85% of training data for 2 iterations to get the finetuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Training Moved to [`research/04_model_trainer.ipynb`](https://github.com/shriadke/JobClassification/blob/master/research/04_model_trainer.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store model to hub\n",
    "sbert_model.save_to_hub(\n",
    "    \"shriadke/adept-job-msmarco-distilbert-base-v4\", \n",
    "    organization=\"\",\n",
    "    exist_ok=True, \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Evaluation Moved to [`research/05_model_evaluation.ipynb`](https://github.com/shriadke/JobClassification/blob/master/research/05_model_evaluation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Embeddings for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ONETs to corresponding embeddings and save it to pickle\n",
    "onet_embd_df = get_onet_embeddings(onet_embd_path=None, save_embd=True, save_path=\"../data/processed/embeddings/onet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ONET_NAME</th>\n",
       "      <th>ONET_EMBD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accountants and Auditors</td>\n",
       "      <td>[-0.81990623, 0.36192688, 0.5624391, 0.611101,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actors</td>\n",
       "      <td>[0.33532786, 0.5877481, -0.66991186, -0.950445...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Actuaries</td>\n",
       "      <td>[-0.1727865, -0.7303153, 0.28830737, -0.694946...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Acupuncturists</td>\n",
       "      <td>[-0.4327046, 0.42423505, -0.465206, 0.6233071,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acute Care Nurses</td>\n",
       "      <td>[-0.562286, -0.8178085, -0.08535522, -1.267615...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ONET_NAME                                          ONET_EMBD\n",
       "0  Accountants and Auditors  [-0.81990623, 0.36192688, 0.5624391, 0.611101,...\n",
       "1                    Actors  [0.33532786, 0.5877481, -0.66991186, -0.950445...\n",
       "2                 Actuaries  [-0.1727865, -0.7303153, 0.28830737, -0.694946...\n",
       "3            Acupuncturists  [-0.4327046, 0.42423505, -0.465206, 0.6233071,...\n",
       "4         Acute Care Nurses  [-0.562286, -0.8178085, -0.08535522, -1.267615..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "onet_embd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1017\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ONET_NAME</th>\n",
       "      <th>ONET_EMBD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accountants and Auditors</td>\n",
       "      <td>[-0.81990623, 0.36192688, 0.5624391, 0.611101,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actors</td>\n",
       "      <td>[0.33532786, 0.5877481, -0.66991186, -0.950445...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Actuaries</td>\n",
       "      <td>[-0.1727865, -0.7303153, 0.28830737, -0.694946...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Acupuncturists</td>\n",
       "      <td>[-0.4327046, 0.42423505, -0.465206, 0.6233071,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acute Care Nurses</td>\n",
       "      <td>[-0.562286, -0.8178085, -0.08535522, -1.267615...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ONET_NAME                                          ONET_EMBD\n",
       "0  Accountants and Auditors  [-0.81990623, 0.36192688, 0.5624391, 0.611101,...\n",
       "1                    Actors  [0.33532786, 0.5877481, -0.66991186, -0.950445...\n",
       "2                 Actuaries  [-0.1727865, -0.7303153, 0.28830737, -0.694946...\n",
       "3            Acupuncturists  [-0.4327046, 0.42423505, -0.465206, 0.6233071,...\n",
       "4         Acute Care Nurses  [-0.562286, -0.8178085, -0.08535522, -1.267615..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load saved embeddings from above pickle\n",
    "onet_embd_df = get_onet_embeddings(onet_embd_path=\"./data/processed/embeddings/onet/embd.pkl\", save_embd=False, save_path=None)\n",
    "onet_embd_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute embeddings on train and test data to store them for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a753ff001f6547c4abca31e924296c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/561 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenght of docs:  17927\n",
      "lenght of embd:  17927\n",
      "Total Jobs available:  17927\n"
     ]
    }
   ],
   "source": [
    "temp_train_df = pd.read_csv(\"../artifacts/data_ingestion/raw_train/train_data.csv\")\n",
    "temp_train_df = get_job_embed_df_from_df(job_df=temp_train_df,model_ckpt=\"models/\", save_embd=True, save_path=\"./data/processed/embeddings/job/custom_model_1/train/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55583e94adbb4b4881a09d9ed5d472d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenght of docs:  19394\n",
      "lenght of embd:  19394\n",
      "Total Jobs available:  19394\n"
     ]
    }
   ],
   "source": [
    "temp_test_df = pd.read_csv(\"../artifacts/data_ingestion/raw_test/test_data.csv\")\n",
    "temp_test_df = get_job_embed_df_from_df(job_df=temp_test_df,model_ckpt=\"models/\", save_embd=True, save_path=\"./data/processed/embeddings/job/custom_model_1/test/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Saved dataframe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to Job DF Given\n",
      "Total Jobs available:  19394\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE_RAW</th>\n",
       "      <th>BODY</th>\n",
       "      <th>ONET_NAME</th>\n",
       "      <th>CLEANED_JOB</th>\n",
       "      <th>JOB_EMBD</th>\n",
       "      <th>ONET_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Grocery Order Writer (Buyer / Inventory Replen...</td>\n",
       "      <td>Grocery Order Writer (Buyer / Inventory Replen...</td>\n",
       "      <td>Purchasing Agents, Except Wholesale, Retail, a...</td>\n",
       "      <td>[TTL] grocery order writer buyer inventory rep...</td>\n",
       "      <td>[-0.98015654, -0.2370125, -0.054709036, 0.5909...</td>\n",
       "      <td>794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Superintendent</td>\n",
       "      <td>Apply to this job. \\nThink you're the perfect ...</td>\n",
       "      <td>Education Administrators, Kindergarten through...</td>\n",
       "      <td>[TTL] superintendent [DESC] think perfect cand...</td>\n",
       "      <td>[-0.7707803, 0.068227395, -0.3073499, 0.680984...</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Software Developer IV</td>\n",
       "      <td>Software Developer IV\\nJob Locations\\nUS-NE-Om...</td>\n",
       "      <td>Software Developers</td>\n",
       "      <td>[TTL] software developer iv [DESC] software de...</td>\n",
       "      <td>[-0.62012154, 0.47546214, 0.3680548, 0.3727675...</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Auto Glass Technician</td>\n",
       "      <td>Auto Glass Technician Gerber Collision &amp; Glass...</td>\n",
       "      <td>Automotive Service Technicians and Mechanics</td>\n",
       "      <td>[TTL] auto glass technician [DESC] auto glass ...</td>\n",
       "      <td>[0.7393341, 0.5710749, 0.3004027, 0.2613186, 0...</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Food and Beverage Operations Manager</td>\n",
       "      <td>Food and Beverage Operations Manager Wavetroni...</td>\n",
       "      <td>Food Service Managers</td>\n",
       "      <td>[TTL] food beverage operations manager [DESC] ...</td>\n",
       "      <td>[-0.7466557, 0.78241795, -0.22036402, -0.59654...</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           TITLE_RAW  \\\n",
       "0  Grocery Order Writer (Buyer / Inventory Replen...   \n",
       "1                                     Superintendent   \n",
       "2                              Software Developer IV   \n",
       "3                              Auto Glass Technician   \n",
       "4               Food and Beverage Operations Manager   \n",
       "\n",
       "                                                BODY  \\\n",
       "0  Grocery Order Writer (Buyer / Inventory Replen...   \n",
       "1  Apply to this job. \\nThink you're the perfect ...   \n",
       "2  Software Developer IV\\nJob Locations\\nUS-NE-Om...   \n",
       "3  Auto Glass Technician Gerber Collision & Glass...   \n",
       "4  Food and Beverage Operations Manager Wavetroni...   \n",
       "\n",
       "                                           ONET_NAME  \\\n",
       "0  Purchasing Agents, Except Wholesale, Retail, a...   \n",
       "1  Education Administrators, Kindergarten through...   \n",
       "2                                Software Developers   \n",
       "3       Automotive Service Technicians and Mechanics   \n",
       "4                              Food Service Managers   \n",
       "\n",
       "                                         CLEANED_JOB  \\\n",
       "0  [TTL] grocery order writer buyer inventory rep...   \n",
       "1  [TTL] superintendent [DESC] think perfect cand...   \n",
       "2  [TTL] software developer iv [DESC] software de...   \n",
       "3  [TTL] auto glass technician [DESC] auto glass ...   \n",
       "4  [TTL] food beverage operations manager [DESC] ...   \n",
       "\n",
       "                                            JOB_EMBD  ONET_ID  \n",
       "0  [-0.98015654, -0.2370125, -0.054709036, 0.5909...      794  \n",
       "1  [-0.7707803, 0.068227395, -0.3073499, 0.680984...      277  \n",
       "2  [-0.62012154, 0.47546214, 0.3680548, 0.3727675...      889  \n",
       "3  [0.7393341, 0.5710749, 0.3004027, 0.2613186, 0...       76  \n",
       "4  [-0.7466557, 0.78241795, -0.22036402, -0.59654...      404  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_test_df = test_df.drop([\"ID\", \"POSTED\",\"ONET\"], axis=1)\n",
    "\n",
    "test_job_df = get_job_embed_df_from_df(job_embd_path=\"data/processed/embeddings/job/custom_model_1/test/19394/embd.pkl\")\n",
    "\n",
    "temp_test_df[\"CLEANED_JOB\"] = test_job_df[\"CLEANED_JOB\"]\n",
    "temp_test_df[\"JOB_EMBD\"] = test_job_df[\"JOB_EMBD\"]\n",
    "id_to_onet_dict, onet_to_id_dict = get_onet_dicts()\n",
    "temp_test_df[\"ONET_ID\"] = temp_test_df[\"ONET_NAME\"].apply(lambda x:int(onet_to_id_dict[x]))\n",
    "temp_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Semantic search on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Test DataLoader from saved dataset\n",
    "test_examples = []\n",
    "test_data = dataset['test']\n",
    "\n",
    "n_examples = dataset['test'].num_rows\n",
    "test_queries = {}\n",
    "test_relevant_docs = {}\n",
    "\n",
    "for i in range(n_examples):\n",
    "  example = test_data[i]\n",
    "  test_examples.append(InputExample(texts=[example['job_post'], example['onet_name']]))\n",
    "  test_queries[str(i)] = example['job_post']\n",
    "  test_relevant_docs[str(i)] = onet_to_id_dict[example[\"onet_name\"]]\n",
    "print(f\"We have a {type(test_examples)} of length {len(test_examples)} containing {type(test_examples[0])}'s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries_ids = []\n",
    "for qid in test_queries:\n",
    "    if qid in test_relevant_docs and len(test_relevant_docs[qid]) > 0:\n",
    "        test_queries_ids.append(qid)\n",
    "\n",
    "test_queries = [test_queries[qid] for qid in test_queries_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model as it is\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e2194abb704ad19840cbcaba2e585c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_q_embd = get_embd(test_queries, model=sbert_model)\n",
    "test_hits = util.semantic_search(test_q_embd, np.array(onet_embd_df[\"ONET_EMBD\"].to_list()), top_k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom code to calculate Performance metrics\n",
    "\n",
    "acc_at_k = [1,5,10]\n",
    "num_hits_at_k = {k:0 for k in acc_at_k}\n",
    "\n",
    "precision_recall_at_k = [1,5,10]\n",
    "precisions_at_k = {k:[] for k in precision_recall_at_k}\n",
    "recall_at_k = {k:[] for k in precision_recall_at_k}\n",
    "\n",
    "mrr_at_k = [1,5,10]\n",
    "MRR = {k:0 for k in mrr_at_k}\n",
    "\n",
    "\n",
    "for test_qid in range(len(test_hits)):\n",
    "\n",
    "    qid = test_queries_ids[test_qid]\n",
    "    # Sort scores\n",
    "    top_hits = test_hits[test_qid]\n",
    "    query_relevant_docs = [int(test_relevant_docs[qid])]\n",
    "\n",
    "    # Accuracy@k - We count the result correct, if at least one relevant doc is accross the top-k documents\n",
    "    for k_val in acc_at_k:\n",
    "        for hit in top_hits[0:k_val]:\n",
    "            if hit['corpus_id'] in query_relevant_docs:\n",
    "                num_hits_at_k[k_val] += 1\n",
    "                break\n",
    "\n",
    "    # Precision and Recall@k\n",
    "    for k_val in precision_recall_at_k:\n",
    "        num_correct = 0\n",
    "        for hit in top_hits[0:k_val]:\n",
    "            if hit['corpus_id'] in query_relevant_docs:\n",
    "                num_correct += 1\n",
    "\n",
    "        precisions_at_k[k_val].append(num_correct / k_val)\n",
    "        recall_at_k[k_val].append(num_correct / len(query_relevant_docs))\n",
    "\n",
    "    # MRR@k\n",
    "    for k_val in mrr_at_k:\n",
    "        for rank, hit in enumerate(top_hits[0:k_val]):\n",
    "            if hit['corpus_id'] in query_relevant_docs:\n",
    "                MRR[k_val] += 1.0 / (rank + 1)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy@k {1: 0.5129533678756477, 5: 0.7461139896373057, 10: 0.8134715025906736}\n",
      "precision@k {1: 0.5129533678756477, 5: 0.14922279792746113, 10: 0.08134715025906736}\n",
      "recall@k {1: 0.5129533678756477, 5: 0.7461139896373057, 10: 0.8134715025906736}\n",
      "mrr@k {1: 0.5129533678756477, 5: 0.6051813471502591, 10: 0.6137326260383255}\n"
     ]
    }
   ],
   "source": [
    "for k in num_hits_at_k:\n",
    "    num_hits_at_k[k] /= len(test_queries)\n",
    "\n",
    "for k in precisions_at_k:\n",
    "    precisions_at_k[k] = np.mean(precisions_at_k[k])\n",
    "\n",
    "for k in recall_at_k:\n",
    "    recall_at_k[k] = np.mean(recall_at_k[k])\n",
    "\n",
    "for k in MRR:\n",
    "    MRR[k] /= len(test_queries)\n",
    "\n",
    "op_dict =  {'accuracy@k': num_hits_at_k, 'precision@k': precisions_at_k, 'recall@k': recall_at_k, 'mrr@k': MRR}\n",
    "for key,val in op_dict.items():\n",
    "    print(key, val)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fetch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
